dataset_name: #The name of the dataset to use (via the datasets library.
dataset_config_name: # The configuration name of the dataset to use (via the datasets library.
train_file: # The input training data file (a text file).
validation_file: # An optional input evaluation data file to evaluate the perplexity on (a text file.)
overwrite_cache: false #Overwrite the cached training and evaluation sets
validation_split_percentage: 5 # The percentage of the train set used as validation set in case there's no validation split
max_seq_length: # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.
preprocessing_num_workers: # The number of processes to use for the preprocessing.
mlm_probability: 0.15 #Ratio of tokens to mask for masked language modeling loss
line_by_line: false # Whether distinct lines of text in the dataset are to be handled as distinct sequences.
pad_to_max_length: false
# Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when batching to the maximum length in the batch.
max_train_samples: # For debugging purposes or quicker training, truncate the number of training examples to this value if set.
max_eval_samples: # For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.
